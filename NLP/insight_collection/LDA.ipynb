{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./outputs/reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 741\n"
     ]
    }
   ],
   "source": [
    "love_comments = []\n",
    "hate_comments = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    x = eval(row['attributes'])\n",
    "    if 'love' in x['comment_answers']:\n",
    "        love_comments.append(x['comment_answers']['love']['value'])\n",
    "    if 'hate' in x['comment_answers']:\n",
    "        hate_comments.append(x['comment_answers']['hate']['value'])\n",
    "\n",
    "print(len(love_comments), len(hate_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_love_comments = list(map(str.split, love_comments))\n",
    "# temp = []\n",
    "# for x in split_love_comments:\n",
    "#     x = [word for word in x if word.lower() not in stop_words]\n",
    "#     temp.append(x)\n",
    "# split_love_comments = temp\n",
    "\n",
    "# dictionary = corpora.Dictionary(split_love_comments)\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in split_love_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.016*\"G2\" + 0.008*\"team\" + 0.007*\"platform\" + 0.006*\"always\" + 0.006*\"customer\" + 0.005*\"help\" + 0.004*\"-\" + 0.004*\"reviews\" + 0.004*\"great\" + 0.004*\"software\"\n",
      "Topic: 1 \n",
      "Words: 0.032*\"G2\" + 0.015*\"us\" + 0.014*\"team\" + 0.009*\"customer\" + 0.009*\"easy\" + 0.008*\"reviews\" + 0.007*\"use\" + 0.007*\"platform\" + 0.007*\"get\" + 0.007*\"like\"\n",
      "Topic: 2 \n",
      "Words: 0.018*\"G2\" + 0.011*\"customer\" + 0.010*\"team\" + 0.008*\"review\" + 0.008*\"great\" + 0.008*\"reviews\" + 0.007*\"us\" + 0.005*\"product\" + 0.005*\"always\" + 0.005*\"able\"\n",
      "Topic: 3 \n",
      "Words: 0.021*\"G2\" + 0.014*\"us\" + 0.012*\"reviews\" + 0.011*\"team\" + 0.009*\"great\" + 0.008*\"also\" + 0.008*\"review\" + 0.007*\"like\" + 0.007*\"-\" + 0.007*\"easy\"\n",
      "Topic: 4 \n",
      "Words: 0.015*\"G2\" + 0.014*\"us\" + 0.009*\"reviews\" + 0.008*\"great\" + 0.008*\"help\" + 0.007*\"helpful\" + 0.007*\"get\" + 0.006*\"team\" + 0.006*\"customer\" + 0.006*\"also\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f'Topic: {idx} \\nWords: {topic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.092*\"get\" + 0.090*\"best\" + 0.047*\"experience\" + 0.041*\"using\" + 0.040*\"questions\" + 0.034*\"provide\" + 0.033*\"'re\" + 0.030*\"good\" + 0.030*\"getting\" + 0.020*\"made\"\n",
      "Topic: 1 \n",
      "Words: 0.196*\"reviews\" + 0.138*\"also\" + 0.063*\"new\" + 0.048*\"sales\" + 0.046*\"tool\" + 0.034*\"user\" + 0.024*\"every\" + 0.022*\"helping\" + 0.022*\"go\" + 0.018*\"drive\"\n",
      "Topic: 2 \n",
      "Words: 0.180*\"team\" + 0.096*\"product\" + 0.040*\"leads\" + 0.039*\"competitors\" + 0.034*\"tools\" + 0.033*\"see\" + 0.032*\"manager\" + 0.029*\"working\" + 0.027*\"gives\" + 0.027*\"prospects\"\n",
      "Topic: 3 \n",
      "Words: 0.350*\"g2\" + 0.074*\"helpful\" + 0.063*\"always\" + 0.042*\"software\" + 0.035*\"well\" + 0.025*\"work\" + 0.023*\"information\" + 0.019*\"need\" + 0.018*\"intuitive\" + 0.018*\"allows\"\n",
      "Topic: 4 \n",
      "Words: 0.143*\"customers\" + 0.099*\"success\" + 0.031*\"brand\" + 0.028*\"within\" + 0.028*\"part\" + 0.025*\"vendors\" + 0.024*\"based\" + 0.024*\"people\" + 0.021*\"competitive\" + 0.019*\"feature\"\n",
      "Topic: 5 \n",
      "Words: 0.124*\"'s\" + 0.103*\"review\" + 0.099*\"easy\" + 0.033*\"make\" + 0.028*\"users\" + 0.027*\"content\" + 0.027*\"buyer\" + 0.026*\"better\" + 0.025*\"n't\" + 0.021*\"amazing\"\n",
      "Topic: 6 \n",
      "Words: 0.125*\"customer\" + 0.114*\"marketing\" + 0.079*\"support\" + 0.057*\"really\" + 0.051*\"intent\" + 0.044*\"company\" + 0.031*\"reports\" + 0.028*\"insights\" + 0.028*\"products\" + 0.025*\"ability\"\n",
      "Topic: 7 \n",
      "Words: 0.163*\"great\" + 0.114*\"like\" + 0.054*\"campaigns\" + 0.050*\"way\" + 0.048*\"profile\" + 0.044*\"market\" + 0.029*\"potential\" + 0.026*\"time\" + 0.019*\"create\" + 0.014*\"would\"\n",
      "Topic: 8 \n",
      "Words: 0.079*\"solutions\" + 0.066*\"data\" + 0.060*\"helps\" + 0.052*\"feedback\" + 0.046*\"love\" + 0.039*\"responsive\" + 0.038*\"lot\" + 0.036*\"value\" + 0.036*\"process\" + 0.034*\"able\"\n",
      "Topic: 9 \n",
      "Words: 0.174*\"us\" + 0.125*\"platform\" + 0.096*\"use\" + 0.079*\"help\" + 0.049*\"'ve\" + 0.034*\"business\" + 0.022*\"quick\" + 0.021*\"understand\" + 0.020*\"helped\" + 0.018*\"extremely\"\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Sample text data\n",
    "text_data = []  # Your list of text data\n",
    "for x in split_love_comments:\n",
    "    text_data.extend(x)\n",
    "\n",
    "# Preprocessing: Tokenization, removal of stopwords, punctuation, and lowercase conversion\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Convert text to lowercase and tokenize\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in punctuation]  # Remove stopwords and punctuation\n",
    "    return tokens\n",
    "\n",
    "# Function to generate n-grams of size up to 3\n",
    "def generate_ngrams(tokens):\n",
    "    unigrams = tokens\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    trigrams = list(ngrams(tokens, 3))\n",
    "    return unigrams + bigrams + trigrams\n",
    "\n",
    "# Apply preprocessing and n-gram generation to each document\n",
    "processed_texts = [generate_ngrams(preprocess_text(doc)) for doc in text_data]\n",
    "\n",
    "# Convert each document to a list of strings\n",
    "processed_texts = [[str(token) for token in doc] for doc in processed_texts]\n",
    "\n",
    "# Create a dictionary mapping words to unique ids\n",
    "dictionary = corpora.Dictionary(processed_texts)\n",
    "\n",
    "# Convert documents to a document-term matrix\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_texts]\n",
    "\n",
    "# Train LDA model\n",
    "num_topics = 10\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics and associated words\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f'Topic: {idx} \\nWords: {topic}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
