{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAG: Using Gemma LLM locally for question answering on private data","metadata":{}},{"cell_type":"markdown","source":"In this notebook, our aim is to develop a RAG system utilizing [Google's Gemma](https://ai.google.dev/gemma) model. We'll generate vectors with [Elastic's ELSER](https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html) model and store them in Elasticsearch. Additionally, we'll explore semantic retrieval techniques and present the top search results as a context window to the Gemma model. Furthermore, we'll utilize the [Hugging Face transformer](https://huggingface.co/google/gemma-2b-it) library to load Gemma on a local environment.","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"markdown","source":"**Elastic Credentials** - Create an [Elastic Cloud deployment](https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud) to get all Elastic credentials (`ELASTIC_CLOUD_ID`,` ELASTIC_API_KEY`).\n\n**Hugging Face Token** - To get started with the [Gemma](https://huggingface.co/google/gemma-2b-it) model, it is necessary to agree to the terms on Hugging Face and generate the [access token](https://huggingface.co/docs/hub/en/security-tokens) with `write` role.\n\n**Gemma Model** - We're going to use [gemma-2b-it](https://huggingface.co/google/gemma-2b-it), though Google has released 4 open models. You can use any of them i.e. [gemma-2b](https://huggingface.co/google/gemma-2b), [gemma-7b](https://huggingface.co/google/gemma-7b), [gemma-7b-it](https://huggingface.co/google/gemma-7b-it)","metadata":{}},{"cell_type":"markdown","source":"## Install packages","metadata":{}},{"cell_type":"code","source":"pip install -q -U elasticsearch langchain transformers huggingface_hub torch","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:33:23.111724Z","iopub.execute_input":"2024-04-10T09:33:23.112638Z","iopub.status.idle":"2024-04-10T09:36:11.732892Z","shell.execute_reply.started":"2024-04-10T09:33:23.112606Z","shell.execute_reply":"2024-04-10T09:36:11.730885Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\ntorchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 2.2.2 which is incompatible.\ntorchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 2.2.2 which is incompatible.\ntorchvision 0.16.2+cpu requires torch==2.1.2, but you have torch 2.2.2 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Import packages","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nfrom getpass import getpass\nfrom urllib.request import urlopen\nimport requests\n\nfrom elasticsearch import Elasticsearch, helpers\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import ElasticsearchStore\nfrom langchain import HuggingFacePipeline\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom huggingface_hub import login\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline","metadata":{"execution":{"iopub.status.busy":"2024-04-10T10:46:16.679119Z","iopub.execute_input":"2024-04-10T10:46:16.679807Z","iopub.status.idle":"2024-04-10T10:46:16.690482Z","shell.execute_reply.started":"2024-04-10T10:46:16.679752Z","shell.execute_reply":"2024-04-10T10:46:16.689058Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Get Credentials","metadata":{}},{"cell_type":"code","source":"#ELASTIC_API_KEY = getpass(\"Elastic API Key :\")\nELASTIC_API_KEY = \"QmhaYXg0NEJTbzNMaVQ0eUVDY3U6V1QzMkZtcWVST3k5VlRCbWlqRFpQQQ==\"\n\n#ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID :\")\nELASTIC_CLOUD_ID = \"RAG_G2:dXMtY2VudHJhbDEuZ2NwLmNsb3VkLmVzLmlvJGNiYWE5N2ZlMWQyYTQ3MTdhYjEzYTNlMmRkM2MyZmExJGVjMWM4MWI2NWRiNjQ5YjRiNDlkOTEyOTdiYzU3YTk2\"\n\nelastic_index_name = \"gemma-rag\"","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:36:35.353207Z","iopub.execute_input":"2024-04-10T09:36:35.354530Z","iopub.status.idle":"2024-04-10T09:36:35.359756Z","shell.execute_reply.started":"2024-04-10T09:36:35.354497Z","shell.execute_reply":"2024-04-10T09:36:35.358749Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Add documents","metadata":{}},{"cell_type":"markdown","source":"### Let's download the sample dataset and deserialize the document.","metadata":{}},{"cell_type":"code","source":"url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/datasets/workplace-documents.json\"\n\nresponse = urlopen(url)\n\nworkplace_docs = json.loads(response.read())","metadata":{"execution":{"iopub.status.busy":"2024-04-10T09:36:35.362013Z","iopub.execute_input":"2024-04-10T09:36:35.362321Z","iopub.status.idle":"2024-04-10T09:36:35.718068Z","shell.execute_reply.started":"2024-04-10T09:36:35.362295Z","shell.execute_reply":"2024-04-10T09:36:35.716951Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_response_from_endpoint(url, headers=None):\n    try:\n        response = requests.get(url, headers=headers)\n        # Check if the request was successful (status code 200)\n        if response.status_code == 200:\n            return response.json()  # Return the JSON response\n        else:\n            return None  # Return None if request was not successful\n    except requests.exceptions.RequestException as e:\n        print(\"Error:\", e)\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-04-10T10:45:40.073972Z","iopub.execute_input":"2024-04-10T10:45:40.076828Z","iopub.status.idle":"2024-04-10T10:45:40.088927Z","shell.execute_reply.started":"2024-04-10T10:45:40.076770Z","shell.execute_reply":"2024-04-10T10:45:40.087092Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"API_KEY = \"1da6d9512ad00fc394bd04234bc7358dc9b85d96fa0c56281f710dc9abcef7e5\"\n\ncustom_headers = {\n    \"Authorization\": f\"Token token={API_KEY}\",\n    \"Content Type\": \"application/vnd.api+json\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-10T10:45:41.472872Z","iopub.execute_input":"2024-04-10T10:45:41.473383Z","iopub.status.idle":"2024-04-10T10:45:41.480217Z","shell.execute_reply.started":"2024-04-10T10:45:41.473347Z","shell.execute_reply":"2024-04-10T10:45:41.478756Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def extract_review_info(review):\n    # Extracting attributes\n    attributes = review.get(\"attributes\", {})\n    product_name = attributes.get(\"product_name\", \"\")\n    star_rating = attributes.get(\"star_rating\", \"\")\n    title = attributes.get(\"title\", \"\")\n    user_name = attributes.get(\"user_name\", \"\")\n    submitted_at = attributes.get(\"submitted_at\", \"\")\n    review_source = attributes.get(\"review_source\", \"\")\n    votes_up = attributes.get(\"votes_up\", \"\")\n    votes_down = attributes.get(\"votes_down\", \"\")\n    country_name = attributes.get(\"country_name\", \"\")\n    \n    # Extracting comment answers\n    comment_answers = attributes.get(\"comment_answers\", {})\n    love = comment_answers.get(\"love\", {}).get(\"value\", \"\")\n    hate = comment_answers.get(\"hate\", {}).get(\"value\", \"\")\n    benefits = comment_answers.get(\"benefits\", {}).get(\"value\", \"\")\n    recommendations = comment_answers.get(\"recommendations\", {}).get(\"value\", \"\")\n    \n    # Extracting secondary answers\n    secondary_answers = attributes.get(\"secondary_answers\", {})\n    meets_requirements = secondary_answers.get(\"meets_requirements\", {}).get(\"value\", \"\")\n    ease_of_use = secondary_answers.get(\"ease_of_use\", {}).get(\"value\", \"\")\n    quality_of_support = secondary_answers.get(\"quality_of_support\", {}).get(\"value\", \"\")\n    ease_of_setup = secondary_answers.get(\"ease_of_setup\", {}).get(\"value\", \"\")\n    ease_of_admin = secondary_answers.get(\"ease_of_admin\", {}).get(\"value\", \"\")\n    ease_of_doing_business_with = secondary_answers.get(\"ease_of_doing_business_with\", {}).get(\"value\", \"\")\n    \n    # Constructing content\n    content = f\"\"\"\n    Product Name: {product_name}\n    Star Rating: {star_rating}\n    Title: {title}\n    User Name: {user_name}\n    Submitted At: {submitted_at}\n    Review Source: {review_source}\n    Votes Up: {votes_up}\n    Votes Down: {votes_down}\n    Country Name: {country_name}\n    \n    Love: {love}\n    Hate: {hate}\n    Benefits: {benefits}\n    Recommendations: {recommendations}\n    \n    Secondary Answers:\n    Meets Requirements: {meets_requirements}\n    Ease of Use: {ease_of_use}\n    Quality of Support: {quality_of_support}\n    Ease of Setup: {ease_of_setup}\n    Ease of Admin: {ease_of_admin}\n    Ease of Doing Business With: {ease_of_doing_business_with}\n    \"\"\"\n    \n    # Extracting relationships metadata\n    relationships = review.get(\"relationships\", {})\n    product_link = relationships.get(\"product\", {}).get(\"links\", {}).get(\"self\", \"\")\n    questions_link = relationships.get(\"questions\", {}).get(\"links\", {}).get(\"self\", \"\")\n    answers_link = relationships.get(\"answers\", {}).get(\"links\", {}).get(\"self\", \"\")\n    \n    # Constructing metadata\n    metadata = {\n        \"product_link\": product_link,\n        \"questions_link\": questions_link,\n        \"answers_link\": answers_link\n    }\n    \n    return content.strip(), metadata","metadata":{"execution":{"iopub.status.busy":"2024-04-10T10:45:43.105204Z","iopub.execute_input":"2024-04-10T10:45:43.106383Z","iopub.status.idle":"2024-04-10T10:45:43.125732Z","shell.execute_reply.started":"2024-04-10T10:45:43.106327Z","shell.execute_reply":"2024-04-10T10:45:43.124767Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Split Documents into Passages","metadata":{}},{"cell_type":"code","source":"metadata = []\ncontent = []\n\n\"\"\"for doc in workplace_docs:\n    content.append(doc[\"content\"])\n    metadata.append(\n        {\n            \"name\": doc[\"name\"],\n            \"summary\": doc[\"summary\"],\n            \"rolePermissions\": doc[\"rolePermissions\"],\n        }\n    )\"\"\"\nfor i in range(77):\n    url=f\"https://data.g2.com/api/v1/survey-responses?page%5Bnumber%5D={i+1}&page%5Bsize%5D=10\"\n    response_data=get_response_from_endpoint(url,custom_headers)\n    temp_list=[extract_review_info(review) for review in response_data[\"data\"]]\n    content.extend([review_content for review_content,_ in temp_list])\n    metadata.extend([review_metadata for _,review_metadata in temp_list])","metadata":{"execution":{"iopub.status.busy":"2024-04-10T10:46:32.423988Z","iopub.execute_input":"2024-04-10T10:46:32.424435Z","iopub.status.idle":"2024-04-10T10:46:56.662723Z","shell.execute_reply.started":"2024-04-10T10:46:32.424404Z","shell.execute_reply":"2024-04-10T10:46:56.661665Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0)\ndocs = text_splitter.create_documents(content[:100], metadatas=metadata[:100])","metadata":{"execution":{"iopub.status.busy":"2024-04-10T11:01:45.693221Z","iopub.execute_input":"2024-04-10T11:01:45.695157Z","iopub.status.idle":"2024-04-10T11:01:45.707078Z","shell.execute_reply.started":"2024-04-10T11:01:45.695107Z","shell.execute_reply":"2024-04-10T11:01:45.705764Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"## Index Documents into Elasticsearch using ELSER\n\nBefore we begin indexing, ensure you have [downloaded and deployed the ELSER model](https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-elser.html#download-deploy-elser) in your deployment and is running on the ML node.","metadata":{}},{"cell_type":"code","source":"es = ElasticsearchStore.from_documents(\n    docs,\n    es_cloud_id=ELASTIC_CLOUD_ID,\n    es_api_key=ELASTIC_API_KEY,\n    index_name=elastic_index_name,\n    strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(\n        model_id=\".elser_model_2\"\n    ),\n    request_timeout=\"180s\"\n    ,timeout=\"180s\"\n)\n\nes","metadata":{"execution":{"iopub.status.busy":"2024-04-10T11:01:48.330357Z","iopub.execute_input":"2024-04-10T11:01:48.330882Z","iopub.status.idle":"2024-04-10T11:01:49.493106Z","shell.execute_reply.started":"2024-04-10T11:01:48.330846Z","shell.execute_reply":"2024-04-10T11:01:49.491590Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"<langchain_community.vectorstores.elasticsearch.ElasticsearchStore at 0x7a4dac3533a0>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Hugging Face login","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()#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","metadata":{"execution":{"iopub.status.busy":"2024-04-10T10:09:31.402948Z","iopub.execute_input":"2024-04-10T10:09:31.403497Z","iopub.status.idle":"2024-04-10T10:09:31.436721Z","shell.execute_reply.started":"2024-04-10T10:09:31.403455Z","shell.execute_reply":"2024-04-10T10:09:31.435366Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68429af83a8346ea8b88fdc1ce8c7626"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Initialize the tokenizer with the model (`google/gemma-2b-it`)","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T10:11:02.870354Z","iopub.execute_input":"2024-04-10T10:11:02.870908Z","iopub.status.idle":"2024-04-10T10:12:07.634116Z","shell.execute_reply.started":"2024-04-10T10:11:02.870869Z","shell.execute_reply":"2024-04-10T10:12:07.632847Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1acf7a98af8945ac859428196966b619"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae5c42efaa144805905698ec6caa07b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbbaf457c1364662845a720c996ba4f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe952fffbd574604a1e4bdda07c62458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f2efe1612ac44c18de21665aec5c2df"}},"metadata":{}},{"name":"stderr","text":"Gemma's activation function should be approximate GeLU and not exact GeLU.\nChanging the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e11190818fc2466d90dc0e0edd27197d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83e8569d80e643da8987f99d3cad3149"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5f97e038acc40a7a173c896791c6aec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fae919c924dd41efad5dc47e1e6ae0b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf07145a57c9419899e3ac7e2f7a12be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/888 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31cb8a3255d54c05a3ebcbfdbc35cd40"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Create a `text-generation` pipeline and initialize with LLM","metadata":{}},{"cell_type":"code","source":"pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=1024,\n)\n\nllm = HuggingFacePipeline(\n    pipeline=pipe,\n    model_kwargs={\"temperature\": 0.7},\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T10:13:19.316533Z","iopub.execute_input":"2024-04-10T10:13:19.317510Z","iopub.status.idle":"2024-04-10T10:13:19.389106Z","shell.execute_reply.started":"2024-04-10T10:13:19.317471Z","shell.execute_reply":"2024-04-10T10:13:19.388036Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Format Docs","metadata":{}},{"cell_type":"code","source":"def format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T10:13:22.174718Z","iopub.execute_input":"2024-04-10T10:13:22.176124Z","iopub.status.idle":"2024-04-10T10:13:22.182326Z","shell.execute_reply.started":"2024-04-10T10:13:22.176061Z","shell.execute_reply":"2024-04-10T10:13:22.181179Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Create a chain using Prompt template","metadata":{}},{"cell_type":"code","source":"retriever = es.as_retriever(search_kwargs={\"k\": 5})\n\ntemplate = \"\"\"Answer the question based only on the following context:\\n\n\n{context}\n\nQuestion: {question}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)\n\nchain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T11:02:29.928056Z","iopub.execute_input":"2024-04-10T11:02:29.928546Z","iopub.status.idle":"2024-04-10T11:02:29.938040Z","shell.execute_reply.started":"2024-04-10T11:02:29.928515Z","shell.execute_reply":"2024-04-10T11:02:29.936815Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## Ask question","metadata":{}},{"cell_type":"code","source":"chain.invoke(\"Explain about G2 and things people like and hate about it\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T11:02:49.872375Z","iopub.execute_input":"2024-04-10T11:02:49.873485Z","iopub.status.idle":"2024-04-10T11:05:07.719015Z","shell.execute_reply.started":"2024-04-10T11:02:49.873441Z","shell.execute_reply":"2024-04-10T11:05:07.717747Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"'Human: Answer the question based only on the following context:\\n\\n\\nThere are major upsides to using G2 not just for competitive comparisons but also to reflect on what your customer like and/or dislike. This can be great feedback for customer success teams and product teams! It also helps me understand what I need to do a better job of explaining specific features/use cases.\\n\\nThere are major upsides to using G2 not just for competitive comparisons but also to reflect on what your customer like and/or dislike. This can be great feedback for customer success teams and product teams! It also helps me understand what I need to do a better job of explaining specific features/use cases.\\n\\nThere are major upsides to using G2 not just for competitive comparisons but also to reflect on what your customer like and/or dislike. This can be great feedback for customer success teams and product teams! It also helps me understand what I need to do a better job of explaining specific features/use cases.\\n\\nThere are major upsides to using G2 not just for competitive comparisons but also to reflect on what your customer like and/or dislike. This can be great feedback for customer success teams and product teams! It also helps me understand what I need to do a better job of explaining specific features/use cases.\\n\\nHate: There are so many useful features – there is not much to dislike.  And I think the vision and roadmap can take G2 to even broader value for us.  I guess the most difficult part is change management on our side – i.e. how do we get our teams to more habitually use the G2 features and information in all of our internal sales, marketing, support, and product processes.  \\n    Benefits: There is nothing more powerful or credible than first-hand experience from a real customer.  Our biggest benefit is the ability to utilize the customer’s voice to tell our story and articulate our value.  We also get the benefit of users telling us directly where we can improve, so it serves as a great input to our product and support organizations.\\n\\nQuestion: Explain about G2 and things people like and hate about it\\nG2 is a platform that helps people to understand each other better.  It is a place where people can share their thoughts and feelings, and where they can learn from each other.  G2 is a place where people can connect with each other, and where they can build a better world.\\n\\nPeople like G2 because it is a place where they can share their thoughts and feelings, and where they can learn from each other.  G2 is a place where people can connect with each other, and where they can build a better world.\\n\\nPeople hate G2 because it is a place where people can share their thoughts and feelings, and where they can learn from each other.  G2 is a place where people can connect with each other, and where they can build a better world.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}